{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b26e31a1-d4b2-4825-bfdf-5ce5d88a0440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1:   0%|                                                                 | 44/636793 [00:34<136:42:48,  1.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 139\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m tqdm(chunk\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(chunk), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mchunk_size\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    138\u001b[0m     rows\u001b[38;5;241m.\u001b[39mappend(scrape_case(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCase Number\u001b[39m\u001b[38;5;124m\"\u001b[39m], row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcourt_number\u001b[39m\u001b[38;5;124m\"\u001b[39m], row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCase Type\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m--> 139\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep_seconds)\n\u001b[0;32m    141\u001b[0m df_out \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rows)\n\u001b[0;32m    142\u001b[0m write_header \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_file)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "input_file = \"Harris_CivilHearingsFilings_Jan22Dec25_CLEAN.csv\"\n",
    "output_file = \"Harris_CivilHearingsFilings_Jan22Dec25_GRANULARv2.csv\"\n",
    "\n",
    "chunk_size = 1000000\n",
    "sleep_seconds = 0.001\n",
    "\n",
    "df_cases = pd.read_csv(\n",
    "    input_file,\n",
    "    usecols=[\"Case Number\", \"court_number\", \"Case Type\"],\n",
    "    dtype={\"Case Number\": \"string\", \"court_number\": \"string\", \"Case Type\": \"string\"},\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "df_cases = df_cases.dropna(subset=[\"Case Number\"])\n",
    "df_cases[\"Case Number\"] = df_cases[\"Case Number\"].astype(\"string\").str.strip()\n",
    "\n",
    "cases_to_scrape = df_cases.drop_duplicates(subset=[\"Case Number\"]).reset_index(drop=True)\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "def scrape_case(case_number, court_number, case_type):\n",
    "    case_number = str(case_number).strip()\n",
    "    url = f\"https://jpwebsite.harriscountytx.gov/CaseInfo/GetCaseInfo?case={case_number}\"\n",
    "\n",
    "    last_err = None\n",
    "    html = None\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            r = session.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            html = r.text\n",
    "            break\n",
    "        except requests.RequestException as e:\n",
    "            last_err = e\n",
    "            time.sleep(1)\n",
    "\n",
    "    result = {\n",
    "        \"Case Number\": case_number,\n",
    "        \"court_number\": court_number,\n",
    "        \"Case Type\": case_type,\n",
    "        \"Plaintiff\": \"\",\n",
    "        \"Defendant\": \"\",\n",
    "        \"Receiver\": \"\"\n",
    "    }\n",
    "\n",
    "    if html is None:\n",
    "        result[\"Error\"] = str(last_err)\n",
    "        return result\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def extract_field(label):\n",
    "        tag = soup.find(\"span\", string=lambda s: s and s.strip() == label)\n",
    "        if tag:\n",
    "            sib = tag.find_next_sibling(\"span\")\n",
    "            if sib:\n",
    "                return sib.get_text(strip=True)\n",
    "        return \"\"\n",
    "\n",
    "    result[\"Filed Date\"] = extract_field(\"Filed Date:\")\n",
    "    result[\"Case Status\"] = extract_field(\"Case Status:\")\n",
    "    result[\"Nature of Claim\"] = extract_field(\"Nature of Claim:\")\n",
    "    result[\"Disposition Desc\"] = extract_field(\"Disposition:\")\n",
    "    result[\"Disposition Date\"] = extract_field(\"Disposition Date:\")\n",
    "    result[\"Judgment Date\"] = extract_field(\"Judgment Date:\")\n",
    "    result[\"Claim Amount\"] = extract_field(\"Claim Amount:\")\n",
    "\n",
    "    party_blocks = soup.select(\"#partyInfo div.even, #partyInfo div.odd\")\n",
    "    for block in party_blocks:\n",
    "        role = block.find(\"span\", string=lambda s: s and s.strip() == \"Party Type:\")\n",
    "        name = block.find(\"span\", string=lambda s: s and s.strip() == \"Party Name:\")\n",
    "\n",
    "        role_span = role.find_next_sibling(\"span\") if role else None\n",
    "        name_span = name.find_next_sibling(\"span\") if name else None\n",
    "\n",
    "        if not (role_span and name_span):\n",
    "            continue\n",
    "\n",
    "        role_value = role_span.get_text(strip=True)\n",
    "        name_value = name_span.get_text(strip=True)\n",
    "\n",
    "        if role_value == \"Plaintiff\" and not result[\"Plaintiff\"]:\n",
    "            result[\"Plaintiff\"] = name_value\n",
    "        elif role_value == \"Defendant\" and not result[\"Defendant\"]:\n",
    "            result[\"Defendant\"] = name_value\n",
    "        elif role_value == \"Receiver\" and not result[\"Receiver\"]:\n",
    "            result[\"Receiver\"] = name_value\n",
    "\n",
    "    hearing_blocks = soup.select(\"#eventInfo div.even, #eventInfo div.odd\")[:10]\n",
    "    for i, block in enumerate(hearing_blocks, start=1):\n",
    "        desc = block.find(\"span\", string=lambda s: s and s.strip() == \"Hearing Description:\")\n",
    "        date = block.find(\"span\", string=lambda s: s and s.strip() == \"Hearing Date/Time:\")\n",
    "\n",
    "        result[f\"Hearing {i} Description\"] = (\n",
    "            desc.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if desc and desc.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "        result[f\"Hearing {i} Date\"] = (\n",
    "            date.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if date and date.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "\n",
    "    event_blocks = soup.select(\"#filingInfo div.even, #filingInfo div.odd\")[:25]\n",
    "    for i, block in enumerate(event_blocks, start=1):\n",
    "        ev = block.find(\"span\", string=lambda s: s and s.strip().startswith(\"Event\"))\n",
    "        dt = block.find(\"span\", string=lambda s: s and s.strip() == \"Date Added:\")\n",
    "\n",
    "        result[f\"Event {i} Description\"] = (\n",
    "            ev.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if ev and ev.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "        result[f\"Event {i} Date\"] = (\n",
    "            dt.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if dt and dt.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "\n",
    "    if not result.get(\"Filed Date\") or not result.get(\"Plaintiff\"):\n",
    "        result[\"Error\"] = result.get(\"Error\", \"Missing core data\")\n",
    "\n",
    "    return result\n",
    "\n",
    "total = len(cases_to_scrape)\n",
    "for start in range(0, total, chunk_size):\n",
    "    chunk = cases_to_scrape.iloc[start:start + chunk_size]\n",
    "    rows = []\n",
    "    for _, row in tqdm(chunk.iterrows(), total=len(chunk), desc=f\"Chunk {start // chunk_size + 1}\"):\n",
    "        rows.append(scrape_case(row[\"Case Number\"], row[\"court_number\"], row[\"Case Type\"]))\n",
    "        time.sleep(sleep_seconds)\n",
    "\n",
    "    df_out = pd.DataFrame(rows)\n",
    "    write_header = not os.path.exists(output_file)\n",
    "    df_out.to_csv(output_file, mode=\"a\", index=False, header=write_header)\n",
    "\n",
    "print(f\"✅ Scraped {total} unique cases into '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3669cdc1-3125-43ee-a484-922b21b08332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1:  24%|██████████████▍                                             | 68728/284846 [17:34:19<45:18:22,  1.33it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "input_file = \"Harris_CivilHearingsFilings_Jan22Dec25_CLEAN.csv\"\n",
    "output_file = \"Harris_HearingsFilings_Jan22Dec25_GRANULARv2_DEBTCLAIM.csv\"\n",
    "\n",
    "chunk_size = 1_000_000\n",
    "sleep_seconds = 0.001\n",
    "\n",
    "# -----------------------------\n",
    "# Load + filter FIRST (Debt Claim only)\n",
    "# -----------------------------\n",
    "df_cases = pd.read_csv(\n",
    "    input_file,\n",
    "    usecols=[\"Case Number\", \"court_number\", \"Case Type\"],\n",
    "    dtype={\"Case Number\": \"string\", \"court_number\": \"string\", \"Case Type\": \"string\"},\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "df_cases[\"Case Type\"] = df_cases[\"Case Type\"].astype(\"string\").str.strip()\n",
    "df_cases = df_cases.loc[df_cases[\"Case Type\"].eq(\"Debt Claim\")].copy()\n",
    "\n",
    "df_cases = df_cases.dropna(subset=[\"Case Number\"])\n",
    "df_cases[\"Case Number\"] = df_cases[\"Case Number\"].astype(\"string\").str.strip()\n",
    "\n",
    "cases_to_scrape = df_cases.drop_duplicates(subset=[\"Case Number\"]).reset_index(drop=True)\n",
    "\n",
    "# Fresh output each run\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "def scrape_case(case_number, court_number, case_type):\n",
    "    case_number = str(case_number).strip()\n",
    "    url = f\"https://jpwebsite.harriscountytx.gov/CaseInfo/GetCaseInfo?case={case_number}\"\n",
    "\n",
    "    last_err = None\n",
    "    html = None\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            r = session.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            html = r.text\n",
    "            break\n",
    "        except requests.RequestException as e:\n",
    "            last_err = e\n",
    "            time.sleep(1)\n",
    "\n",
    "    result = {\n",
    "        \"Case Number\": case_number,\n",
    "        \"court_number\": court_number,\n",
    "        \"Case Type\": case_type,\n",
    "        \"Plaintiff\": \"\",\n",
    "        \"Defendant\": \"\",\n",
    "        \"Receiver\": \"\"\n",
    "    }\n",
    "\n",
    "    if html is None:\n",
    "        result[\"Error\"] = str(last_err)\n",
    "        return result\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def extract_field(label):\n",
    "        tag = soup.find(\"span\", string=lambda s: s and s.strip() == label)\n",
    "        if tag:\n",
    "            sib = tag.find_next_sibling(\"span\")\n",
    "            if sib:\n",
    "                return sib.get_text(strip=True)\n",
    "        return \"\"\n",
    "\n",
    "    result[\"Filed Date\"] = extract_field(\"Filed Date:\")\n",
    "    result[\"Case Status\"] = extract_field(\"Case Status:\")\n",
    "    result[\"Nature of Claim\"] = extract_field(\"Nature of Claim:\")\n",
    "    result[\"Disposition Desc\"] = extract_field(\"Disposition:\")\n",
    "    result[\"Disposition Date\"] = extract_field(\"Disposition Date:\")\n",
    "    result[\"Judgment Date\"] = extract_field(\"Judgment Date:\")\n",
    "    result[\"Claim Amount\"] = extract_field(\"Claim Amount:\")\n",
    "\n",
    "    party_blocks = soup.select(\"#partyInfo div.even, #partyInfo div.odd\")\n",
    "    for block in party_blocks:\n",
    "        role = block.find(\"span\", string=lambda s: s and s.strip() == \"Party Type:\")\n",
    "        name = block.find(\"span\", string=lambda s: s and s.strip() == \"Party Name:\")\n",
    "\n",
    "        role_span = role.find_next_sibling(\"span\") if role else None\n",
    "        name_span = name.find_next_sibling(\"span\") if name else None\n",
    "\n",
    "        if not (role_span and name_span):\n",
    "            continue\n",
    "\n",
    "        role_value = role_span.get_text(strip=True)\n",
    "        name_value = name_span.get_text(strip=True)\n",
    "\n",
    "        if role_value == \"Plaintiff\" and not result[\"Plaintiff\"]:\n",
    "            result[\"Plaintiff\"] = name_value\n",
    "        elif role_value == \"Defendant\" and not result[\"Defendant\"]:\n",
    "            result[\"Defendant\"] = name_value\n",
    "        elif role_value == \"Receiver\" and not result[\"Receiver\"]:\n",
    "            result[\"Receiver\"] = name_value\n",
    "\n",
    "    hearing_blocks = soup.select(\"#eventInfo div.even, #eventInfo div.odd\")[:10]\n",
    "    for i, block in enumerate(hearing_blocks, start=1):\n",
    "        desc = block.find(\"span\", string=lambda s: s and s.strip() == \"Hearing Description:\")\n",
    "        date = block.find(\"span\", string=lambda s: s and s.strip() == \"Hearing Date/Time:\")\n",
    "\n",
    "        result[f\"Hearing {i} Description\"] = (\n",
    "            desc.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if desc and desc.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "        result[f\"Hearing {i} Date\"] = (\n",
    "            date.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if date and date.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "\n",
    "    event_blocks = soup.select(\"#filingInfo div.even, #filingInfo div.odd\")[:25]\n",
    "    for i, block in enumerate(event_blocks, start=1):\n",
    "        ev = block.find(\"span\", string=lambda s: s and s.strip().startswith(\"Event\"))\n",
    "        dt = block.find(\"span\", string=lambda s: s and s.strip() == \"Date Added:\")\n",
    "\n",
    "        result[f\"Event {i} Description\"] = (\n",
    "            ev.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if ev and ev.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "        result[f\"Event {i} Date\"] = (\n",
    "            dt.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if dt and dt.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "\n",
    "    if not result.get(\"Filed Date\") or not result.get(\"Plaintiff\"):\n",
    "        result[\"Error\"] = result.get(\"Error\", \"Missing core data\")\n",
    "\n",
    "    return result\n",
    "\n",
    "total = len(cases_to_scrape)\n",
    "for start in range(0, total, chunk_size):\n",
    "    chunk = cases_to_scrape.iloc[start:start + chunk_size]\n",
    "    rows = []\n",
    "\n",
    "    for _, row in tqdm(chunk.iterrows(), total=len(chunk), desc=f\"Chunk {start // chunk_size + 1}\"):\n",
    "        rows.append(scrape_case(row[\"Case Number\"], row[\"court_number\"], row[\"Case Type\"]))\n",
    "        time.sleep(sleep_seconds)\n",
    "\n",
    "    df_out = pd.DataFrame(rows)\n",
    "    write_header = not os.path.exists(output_file)\n",
    "    df_out.to_csv(output_file, mode=\"a\", index=False, header=write_header)\n",
    "\n",
    "print(f\"✅ Scraped {total} unique Debt Claim cases into '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2225d098-94a4-4274-918a-9ec06397d614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1:   2%|█▌                                                               | 1365/55648 [18:28<12:14:27,  1.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 158\u001b[0m\n\u001b[0;32m    155\u001b[0m rows \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m tqdm(chunk\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(chunk), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mchunk_size\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 158\u001b[0m     rows\u001b[38;5;241m.\u001b[39mappend(scrape_case(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCase Number\u001b[39m\u001b[38;5;124m\"\u001b[39m], row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcourt_number\u001b[39m\u001b[38;5;124m\"\u001b[39m], row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCase Type\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m    159\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep_seconds)\n\u001b[0;32m    161\u001b[0m df_out \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rows)\n",
      "Cell \u001b[1;32mIn[10], line 60\u001b[0m, in \u001b[0;36mscrape_case\u001b[1;34m(case_number, court_number, case_type)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m         r \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget(url, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     61\u001b[0m         r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     62\u001b[0m         html \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "input_file = \"Harris_CivilHearingsFilings_Jan22Dec25_CLEAN.csv\"\n",
    "output_file = \"Harris_CivilHearingsFilings_Jan22Dec25_GRANULARv2_DEBTCLAIM_2023.csv\"\n",
    "\n",
    "chunk_size = 1_000_000\n",
    "sleep_seconds = 0.001\n",
    "\n",
    "# -----------------------------\n",
    "# Load + filter FIRST (Debt Claim + Case File Date in 2023 only)\n",
    "# -----------------------------\n",
    "df_cases = pd.read_csv(\n",
    "    input_file,\n",
    "    usecols=[\"Case Number\", \"court_number\", \"Case Type\", \"Case File Date\"],\n",
    "    dtype={\n",
    "        \"Case Number\": \"string\",\n",
    "        \"court_number\": \"string\",\n",
    "        \"Case Type\": \"string\",\n",
    "        \"Case File Date\": \"string\",\n",
    "    },\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "# Clean strings\n",
    "df_cases[\"Case Type\"] = df_cases[\"Case Type\"].astype(\"string\").str.strip()\n",
    "df_cases[\"Case Number\"] = df_cases[\"Case Number\"].astype(\"string\").str.strip()\n",
    "\n",
    "# Parse Case File Date and filter to calendar year 2023\n",
    "df_cases[\"Case File Date\"] = pd.to_datetime(df_cases[\"Case File Date\"], errors=\"coerce\")\n",
    "df_cases = df_cases.dropna(subset=[\"Case Number\", \"Case File Date\"])\n",
    "\n",
    "df_cases = df_cases.loc[\n",
    "    (df_cases[\"Case Type\"].eq(\"Debt Claim\")) &\n",
    "    (df_cases[\"Case File Date\"].dt.year.eq(2023))\n",
    "].copy()\n",
    "\n",
    "# De-dupe after filtering\n",
    "cases_to_scrape = df_cases.drop_duplicates(subset=[\"Case Number\"]).reset_index(drop=True)\n",
    "\n",
    "# Fresh output each run\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "def scrape_case(case_number, court_number, case_type):\n",
    "    case_number = str(case_number).strip()\n",
    "    url = f\"https://jpwebsite.harriscountytx.gov/CaseInfo/GetCaseInfo?case={case_number}\"\n",
    "\n",
    "    last_err = None\n",
    "    html = None\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            r = session.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            html = r.text\n",
    "            break\n",
    "        except requests.RequestException as e:\n",
    "            last_err = e\n",
    "            time.sleep(1)\n",
    "\n",
    "    result = {\n",
    "        \"Case Number\": case_number,\n",
    "        \"court_number\": court_number,\n",
    "        \"Case Type\": case_type,\n",
    "        \"Plaintiff\": \"\",\n",
    "        \"Defendant\": \"\",\n",
    "        \"Receiver\": \"\"\n",
    "    }\n",
    "\n",
    "    if html is None:\n",
    "        result[\"Error\"] = str(last_err)\n",
    "        return result\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def extract_field(label):\n",
    "        tag = soup.find(\"span\", string=lambda s: s and s.strip() == label)\n",
    "        if tag:\n",
    "            sib = tag.find_next_sibling(\"span\")\n",
    "            if sib:\n",
    "                return sib.get_text(strip=True)\n",
    "        return \"\"\n",
    "\n",
    "    result[\"Filed Date\"] = extract_field(\"Filed Date:\")\n",
    "    result[\"Case Status\"] = extract_field(\"Case Status:\")\n",
    "    result[\"Nature of Claim\"] = extract_field(\"Nature of Claim:\")\n",
    "    result[\"Disposition Desc\"] = extract_field(\"Disposition:\")\n",
    "    result[\"Disposition Date\"] = extract_field(\"Disposition Date:\")\n",
    "    result[\"Judgment Date\"] = extract_field(\"Judgment Date:\")\n",
    "    result[\"Claim Amount\"] = extract_field(\"Claim Amount:\")\n",
    "\n",
    "    party_blocks = soup.select(\"#partyInfo div.even, #partyInfo div.odd\")\n",
    "    for block in party_blocks:\n",
    "        role = block.find(\"span\", string=lambda s: s and s.strip() == \"Party Type:\")\n",
    "        name = block.find(\"span\", string=lambda s: s and s.strip() == \"Party Name:\")\n",
    "\n",
    "        role_span = role.find_next_sibling(\"span\") if role else None\n",
    "        name_span = name.find_next_sibling(\"span\") if name else None\n",
    "        if not (role_span and name_span):\n",
    "            continue\n",
    "\n",
    "        role_value = role_span.get_text(strip=True)\n",
    "        name_value = name_span.get_text(strip=True)\n",
    "\n",
    "        if role_value == \"Plaintiff\" and not result[\"Plaintiff\"]:\n",
    "            result[\"Plaintiff\"] = name_value\n",
    "        elif role_value == \"Defendant\" and not result[\"Defendant\"]:\n",
    "            result[\"Defendant\"] = name_value\n",
    "        elif role_value == \"Receiver\" and not result[\"Receiver\"]:\n",
    "            result[\"Receiver\"] = name_value\n",
    "\n",
    "    hearing_blocks = soup.select(\"#eventInfo div.even, #eventInfo div.odd\")[:10]\n",
    "    for i, block in enumerate(hearing_blocks, start=1):\n",
    "        desc = block.find(\"span\", string=lambda s: s and s.strip() == \"Hearing Description:\")\n",
    "        date = block.find(\"span\", string=lambda s: s and s.strip() == \"Hearing Date/Time:\")\n",
    "\n",
    "        result[f\"Hearing {i} Description\"] = (\n",
    "            desc.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if desc and desc.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "        result[f\"Hearing {i} Date\"] = (\n",
    "            date.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if date and date.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "\n",
    "    event_blocks = soup.select(\"#filingInfo div.even, #filingInfo div.odd\")[:25]\n",
    "    for i, block in enumerate(event_blocks, start=1):\n",
    "        ev = block.find(\"span\", string=lambda s: s and s.strip().startswith(\"Event\"))\n",
    "        dt = block.find(\"span\", string=lambda s: s and s.strip() == \"Date Added:\")\n",
    "\n",
    "        result[f\"Event {i} Description\"] = (\n",
    "            ev.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if ev and ev.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "        result[f\"Event {i} Date\"] = (\n",
    "            dt.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if dt and dt.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "\n",
    "    if not result.get(\"Filed Date\") or not result.get(\"Plaintiff\"):\n",
    "        result[\"Error\"] = result.get(\"Error\", \"Missing core data\")\n",
    "\n",
    "    return result\n",
    "\n",
    "total = len(cases_to_scrape)\n",
    "for start in range(0, total, chunk_size):\n",
    "    chunk = cases_to_scrape.iloc[start:start + chunk_size]\n",
    "    rows = []\n",
    "\n",
    "    for _, row in tqdm(chunk.iterrows(), total=len(chunk), desc=f\"Chunk {start // chunk_size + 1}\"):\n",
    "        rows.append(scrape_case(row[\"Case Number\"], row[\"court_number\"], row[\"Case Type\"]))\n",
    "        time.sleep(sleep_seconds)\n",
    "\n",
    "    df_out = pd.DataFrame(rows)\n",
    "    write_header = not os.path.exists(output_file)\n",
    "    df_out.to_csv(output_file, mode=\"a\", index=False, header=write_header)\n",
    "\n",
    "print(f\"✅ Scraped {total} unique Debt Claim cases filed in 2023 into '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d1ae6-2f88-495e-beb6-b4882213f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "input_file = \"JP12_CivilHearingsFilings_Jan15Dec25_CLEAN.csv\"\n",
    "output_file = \"JP12_CivilHearingsFilings_Jan15Dec25_GRANULAR.csv\"\n",
    "\n",
    "chunk_size = 1000000\n",
    "sleep_seconds = 0.5\n",
    "\n",
    "df_cases = pd.read_csv(\n",
    "    input_file,\n",
    "    usecols=[\"Case Number\", \"court_number\", \"Case Type\"],\n",
    "    dtype={\"Case Number\": \"string\", \"court_number\": \"string\", \"Case Type\": \"string\"},\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "df_cases = df_cases.dropna(subset=[\"Case Number\"])\n",
    "df_cases[\"Case Number\"] = df_cases[\"Case Number\"].astype(\"string\").str.strip()\n",
    "\n",
    "cases_to_scrape = df_cases.drop_duplicates(subset=[\"Case Number\"]).reset_index(drop=True)\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "def scrape_case(case_number, court_number, case_type):\n",
    "    case_number = str(case_number).strip()\n",
    "    url = f\"https://jpwebsite.harriscountytx.gov/CaseInfo/GetCaseInfo?case={case_number}\"\n",
    "\n",
    "    last_err = None\n",
    "    html = None\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            r = session.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            html = r.text\n",
    "            break\n",
    "        except requests.RequestException as e:\n",
    "            last_err = e\n",
    "            time.sleep(1)\n",
    "\n",
    "    result = {\n",
    "        \"Case Number\": case_number,\n",
    "        \"court_number\": court_number,\n",
    "        \"Case Type\": case_type,\n",
    "        \"Plaintiff\": \"\",\n",
    "        \"Defendant\": \"\",\n",
    "        \"Receiver\": \"\"\n",
    "    }\n",
    "\n",
    "    if html is None:\n",
    "        result[\"Error\"] = str(last_err)\n",
    "        return result\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def extract_field(label):\n",
    "        tag = soup.find(\"span\", string=lambda s: s and s.strip() == label)\n",
    "        if tag:\n",
    "            sib = tag.find_next_sibling(\"span\")\n",
    "            if sib:\n",
    "                return sib.get_text(strip=True)\n",
    "        return \"\"\n",
    "\n",
    "    result[\"Filed Date\"] = extract_field(\"Filed Date:\")\n",
    "    result[\"Case Status\"] = extract_field(\"Case Status:\")\n",
    "    result[\"Nature of Claim\"] = extract_field(\"Nature of Claim:\")\n",
    "    result[\"Disposition Desc\"] = extract_field(\"Disposition:\")\n",
    "    result[\"Disposition Date\"] = extract_field(\"Disposition Date:\")\n",
    "    result[\"Judgment Date\"] = extract_field(\"Judgment Date:\")\n",
    "    result[\"Claim Amount\"] = extract_field(\"Claim Amount:\")\n",
    "\n",
    "    party_blocks = soup.select(\"#partyInfo div.even, #partyInfo div.odd\")\n",
    "    for block in party_blocks:\n",
    "        role = block.find(\"span\", string=lambda s: s and s.strip() == \"Party Type:\")\n",
    "        name = block.find(\"span\", string=lambda s: s and s.strip() == \"Party Name:\")\n",
    "\n",
    "        role_span = role.find_next_sibling(\"span\") if role else None\n",
    "        name_span = name.find_next_sibling(\"span\") if name else None\n",
    "\n",
    "        if not (role_span and name_span):\n",
    "            continue\n",
    "\n",
    "        role_value = role_span.get_text(strip=True)\n",
    "        name_value = name_span.get_text(strip=True)\n",
    "\n",
    "        if role_value == \"Plaintiff\" and not result[\"Plaintiff\"]:\n",
    "            result[\"Plaintiff\"] = name_value\n",
    "        elif role_value == \"Defendant\" and not result[\"Defendant\"]:\n",
    "            result[\"Defendant\"] = name_value\n",
    "        elif role_value == \"Receiver\" and not result[\"Receiver\"]:\n",
    "            result[\"Receiver\"] = name_value\n",
    "\n",
    "    hearing_blocks = soup.select(\"#eventInfo div.even, #eventInfo div.odd\")[:10]\n",
    "    for i, block in enumerate(hearing_blocks, start=1):\n",
    "        desc = block.find(\"span\", string=lambda s: s and s.strip() == \"Hearing Description:\")\n",
    "        date = block.find(\"span\", string=lambda s: s and s.strip() == \"Hearing Date/Time:\")\n",
    "\n",
    "        result[f\"Hearing {i} Description\"] = (\n",
    "            desc.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if desc and desc.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "        result[f\"Hearing {i} Date\"] = (\n",
    "            date.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if date and date.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "\n",
    "    event_blocks = soup.select(\"#filingInfo div.even, #filingInfo div.odd\")[:25]\n",
    "    for i, block in enumerate(event_blocks, start=1):\n",
    "        ev = block.find(\"span\", string=lambda s: s and s.strip().startswith(\"Event\"))\n",
    "        dt = block.find(\"span\", string=lambda s: s and s.strip() == \"Date Added:\")\n",
    "\n",
    "        result[f\"Event {i} Description\"] = (\n",
    "            ev.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if ev and ev.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "        result[f\"Event {i} Date\"] = (\n",
    "            dt.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if dt and dt.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "\n",
    "    if not result.get(\"Filed Date\") or not result.get(\"Plaintiff\"):\n",
    "        result[\"Error\"] = result.get(\"Error\", \"Missing core data\")\n",
    "\n",
    "    return result\n",
    "\n",
    "total = len(cases_to_scrape)\n",
    "for start in range(0, total, chunk_size):\n",
    "    chunk = cases_to_scrape.iloc[start:start + chunk_size]\n",
    "    rows = []\n",
    "    for _, row in tqdm(chunk.iterrows(), total=len(chunk), desc=f\"Chunk {start // chunk_size + 1}\"):\n",
    "        rows.append(scrape_case(row[\"Case Number\"], row[\"court_number\"], row[\"Case Type\"]))\n",
    "        time.sleep(sleep_seconds)\n",
    "\n",
    "    df_out = pd.DataFrame(rows)\n",
    "    write_header = not os.path.exists(output_file)\n",
    "    df_out.to_csv(output_file, mode=\"a\", index=False, header=write_header)\n",
    "\n",
    "print(f\"✅ Scraped {total} unique cases into '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a83e78a-2c82-4dff-bc8e-f7ce6c50bfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1: 100%|████████████████████████████████████████████████████████████████| 33469/33469 [23:39:00<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 33469 unique cases into 'JP12_CivilHearingsFilings_Jan22Dec25_GRANULAR.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "input_file = \"JP12_CivilHearingsFilings_Jan22Dec25_CLEAN.csv\"\n",
    "output_file = \"JP12_CivilHearingsFilings_Jan22Dec25_GRANULAR.csv\"\n",
    "\n",
    "chunk_size = 40000\n",
    "sleep_seconds = 0.5\n",
    "\n",
    "df_cases = pd.read_csv(\n",
    "    input_file,\n",
    "    usecols=[\"Case Number\", \"court_number\", \"Case Type\"],\n",
    "    dtype={\"Case Number\": \"string\", \"court_number\": \"string\", \"Case Type\": \"string\"},\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "df_cases = df_cases.dropna(subset=[\"Case Number\"])\n",
    "df_cases[\"Case Number\"] = df_cases[\"Case Number\"].astype(\"string\").str.strip()\n",
    "\n",
    "cases_to_scrape = df_cases.drop_duplicates(subset=[\"Case Number\"]).reset_index(drop=True)\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "def scrape_case(case_number, court_number, case_type):\n",
    "    case_number = str(case_number).strip()\n",
    "    url = f\"https://jpwebsite.harriscountytx.gov/CaseInfo/GetCaseInfo?case={case_number}\"\n",
    "\n",
    "    last_err = None\n",
    "    html = None\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            r = session.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            html = r.text\n",
    "            break\n",
    "        except requests.RequestException as e:\n",
    "            last_err = e\n",
    "            time.sleep(1)\n",
    "\n",
    "    result = {\"Case Number\": case_number, \"court_number\": court_number, \"Case Type\": case_type}\n",
    "\n",
    "    if html is None:\n",
    "        result[\"Error\"] = str(last_err)\n",
    "        return result\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def extract_field(label):\n",
    "        tag = soup.find(\"span\", string=lambda s: s and s.strip() == label)\n",
    "        if tag:\n",
    "            sib = tag.find_next_sibling(\"span\")\n",
    "            if sib:\n",
    "                return sib.get_text(strip=True)\n",
    "        return \"\"\n",
    "\n",
    "    result[\"Filed Date\"] = extract_field(\"Filed Date:\")\n",
    "    result[\"Case Status\"] = extract_field(\"Case Status:\")\n",
    "    result[\"Nature of Claim\"] = extract_field(\"Nature of Claim:\")\n",
    "    result[\"Disposition Desc\"] = extract_field(\"Disposition:\")\n",
    "    result[\"Disposition Date\"] = extract_field(\"Disposition Date:\")\n",
    "    result[\"Judgment Date\"] = extract_field(\"Judgment Date:\")\n",
    "    result[\"Claim Amount\"] = extract_field(\"Claim Amount:\")\n",
    "\n",
    "    party_blocks = soup.select(\"#partyInfo div.even, #partyInfo div.odd\")\n",
    "    for block in party_blocks:\n",
    "        role = block.find(\"span\", string=lambda s: s and s.strip() == \"Party Type:\")\n",
    "        name = block.find(\"span\", string=lambda s: s and s.strip() == \"Party Name:\")\n",
    "        role_span = role.find_next_sibling(\"span\") if role else None\n",
    "        name_span = name.find_next_sibling(\"span\") if name else None\n",
    "        if role_span and name_span:\n",
    "            role_value = role_span.get_text(strip=True)\n",
    "            name_value = name_span.get_text(strip=True)\n",
    "            if role_value == \"Plaintiff\":\n",
    "                result[\"Plaintiff\"] = name_value\n",
    "            elif role_value == \"Defendant\":\n",
    "                result[\"Defendant\"] = name_value\n",
    "\n",
    "    hearing_blocks = soup.select(\"#eventInfo div.even, #eventInfo div.odd\")[:10]\n",
    "    for i, block in enumerate(hearing_blocks, start=1):\n",
    "        desc = block.find(\"span\", string=lambda s: s and s.strip() == \"Hearing Description:\")\n",
    "        date = block.find(\"span\", string=lambda s: s and s.strip() == \"Hearing Date/Time:\")\n",
    "        hres = block.find(\"span\", string=lambda s: s and s.strip() == \"Hearing Result:\")\n",
    "        result[f\"Hearing {i} Description\"] = desc.find_next_sibling(\"span\").get_text(strip=True) if desc and desc.find_next_sibling(\"span\") else \"\"\n",
    "        result[f\"Hearing {i} Date\"] = date.find_next_sibling(\"span\").get_text(strip=True) if date and date.find_next_sibling(\"span\") else \"\"\n",
    "        result[f\"Hearing {i} Result\"] = hres.find_next_sibling(\"span\").get_text(strip=True) if hres and hres.find_next_sibling(\"span\") else \"\"\n",
    "\n",
    "    event_blocks = soup.select(\"#filingInfo div.even, #filingInfo div.odd\")[:25]\n",
    "    for i, block in enumerate(event_blocks, start=1):\n",
    "        ev = block.find(\"span\", string=lambda s: s and s.strip().startswith(\"Event\"))\n",
    "        dt = block.find(\"span\", string=lambda s: s and s.strip() == \"Date Added:\")\n",
    "        result[f\"Event {i} Description\"] = ev.find_next_sibling(\"span\").get_text(strip=True) if ev and ev.find_next_sibling(\"span\") else \"\"\n",
    "        result[f\"Event {i} Date\"] = dt.find_next_sibling(\"span\").get_text(strip=True) if dt and dt.find_next_sibling(\"span\") else \"\"\n",
    "\n",
    "    if not result.get(\"Filed Date\") or not result.get(\"Plaintiff\"):\n",
    "        result[\"Error\"] = result.get(\"Error\", \"Missing core data\")\n",
    "\n",
    "    return result\n",
    "\n",
    "total = len(cases_to_scrape)\n",
    "for start in range(0, total, chunk_size):\n",
    "    chunk = cases_to_scrape.iloc[start:start + chunk_size]\n",
    "    rows = []\n",
    "    for _, row in tqdm(chunk.iterrows(), total=len(chunk), desc=f\"Chunk {start // chunk_size + 1}\"):\n",
    "        rows.append(scrape_case(row[\"Case Number\"], row[\"court_number\"], row[\"Case Type\"]))\n",
    "        time.sleep(sleep_seconds)\n",
    "\n",
    "    df_out = pd.DataFrame(rows)\n",
    "    write_header = not os.path.exists(output_file)\n",
    "    df_out.to_csv(output_file, mode=\"a\", index=False, header=write_header)\n",
    "\n",
    "print(f\"✅ Scraped {total} unique cases into '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "986c458c-fc3c-453c-b27e-9c662747b5ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Remove old file if re-running\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_file):\n\u001b[1;32m---> 17\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(output_file)\u001b[38;5;241m.\u001b[39mcsv\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_eviction_case\u001b[39m(case_number, court_number):\n\u001b[0;32m     20\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://jpwebsite.harriscountytx.gov/CaseInfo/GetCaseInfo?case=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcase_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Load your dataset and sample 100\n",
    "df_cases = pd.read_csv('JP12_CivilHearings_Jan0122_Dec2225_CLEAN.csv', usecols=['Case Number', 'court_number','Case Type'])\n",
    "sampled_cases = df_cases.dropna(subset=['Case Number']).sample(n=100, random_state=42)\n",
    "\n",
    "session = requests.Session(),\n",
    "output_file = 'JP12_CivilHearings_Jan0122_Dec2225_GRANULAR_EVENTS.csv'\n",
    "\n",
    "# Remove old file if re-running\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file).csv\n",
    "\n",
    "def scrape_eviction_case(case_number, court_number):\n",
    "    url = f'https://jpwebsite.harriscountytx.gov/CaseInfo/GetCaseInfo?case={case_number}'\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            response = session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            break\n",
    "        except requests.RequestException as e:\n",
    "            if attempt == 2:\n",
    "                return {'Case Number': case_number, 'court_number': court_number, 'Error': str(e)}\n",
    "            time.sleep(1)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    result = {'Case Number': case_number, 'court_number': court_number}\n",
    "\n",
    "    def extract_field(label):\n",
    "        tag = soup.find('span', string=label)\n",
    "        if tag and tag.find_next_sibling('span'):\n",
    "            return tag.find_next_sibling('span').text.strip()\n",
    "        return ''\n",
    "\n",
    "    result['Filed Date'] = extract_field('Filed Date:')\n",
    "    result['Case Status'] = extract_field('Case Status:')\n",
    "    result['Nature of Claim'] = extract_field('Nature of Claim:')\n",
    "    result['Disposition Desc'] = extract_field('Disposition:')\n",
    "    result['Disposition Date'] = extract_field('Disposition Date:')\n",
    "    result['Judgment Date'] = extract_field('Judgment Date:')\n",
    "    result['Claim Amount'] = extract_field('Claim Amount:')\n",
    "\n",
    "    # Parties\n",
    "    party_blocks = soup.select('#partyInfo div.even, #partyInfo div.odd')\n",
    "    for block in party_blocks:\n",
    "        role = block.find('span', string='Party Type:')\n",
    "        name = block.find('span', string='Party Name:')\n",
    "        if role and name:\n",
    "            role_value = role.find_next_sibling('span').text.strip()\n",
    "            name_value = name.find_next_sibling('span').text.strip()\n",
    "            if role_value == 'Plaintiff':\n",
    "                result['Plaintiff'] = name_value\n",
    "            elif role_value == 'Defendant':\n",
    "                result['Defendant'] = name_value\n",
    "\n",
    "    if not result.get('Filed Date') or not result.get('Plaintiff'):\n",
    "        result['Error'] = 'Missing core data'\n",
    "        return result\n",
    "\n",
    "    # Hearings (up to 10)\n",
    "    hearing_blocks = soup.select('#eventInfo div.even, #eventInfo div.odd')[:10]\n",
    "    for i, block in enumerate(hearing_blocks):\n",
    "        desc = block.find('span', string='Hearing Description:')\n",
    "        date = block.find('span', string='Hearing Date/Time:')\n",
    "        result_text = block.find_all('span')[-3].text.strip() if len(block.find_all('span')) > 3 else ''\n",
    "        result[f'Hearing {i+1} Description'] = desc.find_next_sibling('span').text.strip() if desc else ''\n",
    "        result[f'Hearing {i+1} Date'] = date.find_next_sibling('span').text.strip() if date else ''\n",
    "        result[f'Hearing {i+1} Result'] = result_text\n",
    "\n",
    "    # Events (up to 25)\n",
    "    event_blocks = soup.select('#filingInfo div.even, #filingInfo div.odd')[:25]\n",
    "    for i, block in enumerate(event_blocks):\n",
    "        desc = block.find('span', string=lambda s: s and 'Event' in s)\n",
    "        date = block.find('span', string='Date Added:')\n",
    "        result[f'Event {i+1} Description'] = desc.find_next_sibling('span').text.strip() if desc else ''\n",
    "        result[f'Event {i+1} Date'] = date.find_next_sibling('span').text.strip() if date else ''\n",
    "\n",
    "    return result\n",
    "\n",
    "# Scrape in chunks of 25\n",
    "chunk_size = 25\n",
    "for i in range(0, len(sampled_cases), chunk_size):\n",
    "    chunk = sampled_cases.iloc[i:i+chunk_size]\n",
    "    chunk_results = []\n",
    "    for _, row in tqdm(chunk.iterrows(), total=len(chunk), desc=f\"Processing chunk {i//chunk_size + 1}\"):\n",
    "        case_number = str(row['Case Number'])\n",
    "        court_number = row['court_number']\n",
    "        data = scrape_eviction_case(case_number, court_number)\n",
    "        chunk_results.append(data)\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    df_chunk = pd.DataFrame(chunk_results)\n",
    "    df_chunk.to_csv(output_file, mode='a', index=False, header=not os.path.exists(output_file))\n",
    "\n",
    "print(f\"✅ All chunks scraped and saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e00bce7c-243b-41cb-b92c-bc71b24332e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1: 100%|████████████████████████████████████████████████████████████████| 39243/39243 [16:21:55<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 39243 unique cases into 'JP12_Debt_Jan18Dec25_GRANULAR.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "input_file = \"JP12_CivilHearingsFilings_Jan15Dec25_CLEAN.csv\"\n",
    "output_file = \"JP12_Debt_Jan18Dec25_GRANULAR.csv\"\n",
    "\n",
    "chunk_size = 1000000\n",
    "sleep_seconds = 0.5\n",
    "\n",
    "# Read only what you need (added \"Case File Date\")\n",
    "df_cases = pd.read_csv(\n",
    "    input_file,\n",
    "    usecols=[\"Case Number\", \"court_number\", \"Case Type\", \"Case File Date\"],\n",
    "    dtype={\"Case Number\": \"string\", \"court_number\": \"string\", \"Case Type\": \"string\", \"Case File Date\": \"string\"},\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "# Basic cleaning\n",
    "df_cases = df_cases.dropna(subset=[\"Case Number\"])\n",
    "df_cases[\"Case Number\"] = df_cases[\"Case Number\"].astype(\"string\").str.strip()\n",
    "df_cases[\"Case Type\"] = df_cases[\"Case Type\"].astype(\"string\").str.strip()\n",
    "\n",
    "# --- FILTERS ---\n",
    "# 1) Case Type must be Debt Claim\n",
    "df_cases = df_cases[df_cases[\"Case Type\"] == \"Debt Claim\"].copy()\n",
    "\n",
    "# 2) Case File Date must be Jan 2017 or later\n",
    "# Robust parse: handles MM/DD/YYYY (and similar) safely; unparseable -> NaT -> dropped\n",
    "df_cases[\"Case File Date\"] = pd.to_datetime(df_cases[\"Case File Date\"], errors=\"coerce\")\n",
    "df_cases = df_cases[df_cases[\"Case File Date\"] >= pd.Timestamp(\"2017-01-01\")].copy()\n",
    "\n",
    "# Optional: drop rows where Case File Date couldn't be parsed\n",
    "df_cases = df_cases.dropna(subset=[\"Case File Date\"])\n",
    "\n",
    "cases_to_scrape = df_cases.drop_duplicates(subset=[\"Case Number\"]).reset_index(drop=True)\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "def scrape_case(case_number, court_number, case_type):\n",
    "    case_number = str(case_number).strip()\n",
    "    url = f\"https://jpwebsite.harriscountytx.gov/CaseInfo/GetCaseInfo?case={case_number}\"\n",
    "\n",
    "    last_err = None\n",
    "    html = None\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            r = session.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            html = r.text\n",
    "            break\n",
    "        except requests.RequestException as e:\n",
    "            last_err = e\n",
    "            time.sleep(1)\n",
    "\n",
    "    result = {\n",
    "        \"Case Number\": case_number,\n",
    "        \"court_number\": court_number,\n",
    "        \"Case Type\": case_type,\n",
    "        \"Plaintiff\": \"\",\n",
    "        \"Defendant\": \"\",\n",
    "        \"Receiver\": \"\"\n",
    "    }\n",
    "\n",
    "    if html is None:\n",
    "        result[\"Error\"] = str(last_err)\n",
    "        return result\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def extract_field(label):\n",
    "        tag = soup.find(\"span\", string=lambda s: s and s.strip() == label)\n",
    "        if tag:\n",
    "            sib = tag.find_next_sibling(\"span\")\n",
    "            if sib:\n",
    "                return sib.get_text(strip=True)\n",
    "        return \"\"\n",
    "\n",
    "    result[\"Filed Date\"] = extract_field(\"Filed Date:\")\n",
    "    result[\"Case Status\"] = extract_field(\"Case Status:\")\n",
    "    result[\"Nature of Claim\"] = extract_field(\"Nature of Claim:\")\n",
    "    result[\"Disposition Desc\"] = extract_field(\"Disposition:\")\n",
    "    result[\"Disposition Date\"] = extract_field(\"Disposition Date:\")\n",
    "    result[\"Judgment Date\"] = extract_field(\"Judgment Date:\")\n",
    "    result[\"Claim Amount\"] = extract_field(\"Claim Amount:\")\n",
    "\n",
    "    party_blocks = soup.select(\"#partyInfo div.even, #partyInfo div.odd\")\n",
    "    for block in party_blocks:\n",
    "        role = block.find(\"span\", string=lambda s: s and s.strip() == \"Party Type:\")\n",
    "        name = block.find(\"span\", string=lambda s: s and s.strip() == \"Party Name:\")\n",
    "\n",
    "        role_span = role.find_next_sibling(\"span\") if role else None\n",
    "        name_span = name.find_next_sibling(\"span\") if name else None\n",
    "\n",
    "        if not (role_span and name_span):\n",
    "            continue\n",
    "\n",
    "        role_value = role_span.get_text(strip=True)\n",
    "        name_value = name_span.get_text(strip=True)\n",
    "\n",
    "        if role_value == \"Plaintiff\" and not result[\"Plaintiff\"]:\n",
    "            result[\"Plaintiff\"] = name_value\n",
    "        elif role_value == \"Defendant\" and not result[\"Defendant\"]:\n",
    "            result[\"Defendant\"] = name_value\n",
    "        elif role_value == \"Receiver\" and not result[\"Receiver\"]:\n",
    "            result[\"Receiver\"] = name_value\n",
    "\n",
    "    hearing_blocks = soup.select(\"#eventInfo div.even, #eventInfo div.odd\")[:10]\n",
    "    for i, block in enumerate(hearing_blocks, start=1):\n",
    "        desc = block.find(\"span\", string=lambda s: s and s.strip() == \"Hearing Description:\")\n",
    "        date = block.find(\"span\", string=lambda s: s and s.strip() == \"Hearing Date/Time:\")\n",
    "\n",
    "        result[f\"Hearing {i} Description\"] = (\n",
    "            desc.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if desc and desc.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "        result[f\"Hearing {i} Date\"] = (\n",
    "            date.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if date and date.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "\n",
    "    event_blocks = soup.select(\"#filingInfo div.even, #filingInfo div.odd\")[:25]\n",
    "    for i, block in enumerate(event_blocks, start=1):\n",
    "        ev = block.find(\"span\", string=lambda s: s and s.strip().startswith(\"Event\"))\n",
    "        dt = block.find(\"span\", string=lambda s: s and s.strip() == \"Date Added:\")\n",
    "\n",
    "        result[f\"Event {i} Description\"] = (\n",
    "            ev.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if ev and ev.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "        result[f\"Event {i} Date\"] = (\n",
    "            dt.find_next_sibling(\"span\").get_text(strip=True)\n",
    "            if dt and dt.find_next_sibling(\"span\") else \"\"\n",
    "        )\n",
    "\n",
    "    if not result.get(\"Filed Date\") or not result.get(\"Plaintiff\"):\n",
    "        result[\"Error\"] = result.get(\"Error\", \"Missing core data\")\n",
    "\n",
    "    return result\n",
    "\n",
    "total = len(cases_to_scrape)\n",
    "for start in range(0, total, chunk_size):\n",
    "    chunk = cases_to_scrape.iloc[start:start + chunk_size]\n",
    "    rows = []\n",
    "    for _, row in tqdm(chunk.iterrows(), total=len(chunk), desc=f\"Chunk {start // chunk_size + 1}\"):\n",
    "        rows.append(scrape_case(row[\"Case Number\"], row[\"court_number\"], row[\"Case Type\"]))\n",
    "        time.sleep(sleep_seconds)\n",
    "\n",
    "    df_out = pd.DataFrame(rows)\n",
    "    write_header = not os.path.exists(output_file)\n",
    "    df_out.to_csv(output_file, mode=\"a\", index=False, header=write_header)\n",
    "\n",
    "print(f\"✅ Scraped {total} unique cases into '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4409e31-bcb9-4f7c-a992-2e04893fc6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
