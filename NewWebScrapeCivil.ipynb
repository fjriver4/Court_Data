{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "430d5ec0-2843-4eee-b8fc-a767988c4aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ferna\\AppData\\Local\\Temp\\ipykernel_14520\\3087638131.py:8: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_cases = pd.read_csv('Harris_EV_Filings_2023_042025_CLEAN.csv', usecols=['Case Number', 'court_number'])\n",
      "Scraping eviction cases: 100%|█████████████████████████████████████████████████████████| 10/10 [00:07<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraping complete. Saved to 'Eviction_Cases_Sample_WithCourt.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your case data\n",
    "df_cases = pd.read_csv('Harris_EV_Filings_2023_042025_CLEAN.csv', usecols=['Case Number', 'court_number'])\n",
    "sampled_cases = df_cases.dropna(subset=['Case Number']).sample(n=10, random_state=42)\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "def scrape_eviction_case(case_number, court_number):\n",
    "    url = f'https://jpwebsite.harriscountytx.gov/CaseInfo/GetCaseInfo?case={case_number}'\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            response = session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            break\n",
    "        except requests.RequestException as e:\n",
    "            if attempt == 2:\n",
    "                return {'Case Number': case_number, 'court_number': court_number, 'Error': str(e)}\n",
    "            time.sleep(1)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    result = {'Case Number': case_number, 'court_number': court_number}\n",
    "\n",
    "    def extract_field(label):\n",
    "        tag = soup.find('span', string=label)\n",
    "        if tag and tag.find_next_sibling('span'):\n",
    "            return tag.find_next_sibling('span').text.strip()\n",
    "        return ''\n",
    "\n",
    "    result['Filed Date'] = extract_field('Filed Date:')\n",
    "    result['Case Status'] = extract_field('Case Status:')\n",
    "    result['Nature of Claim'] = extract_field('Nature of Claim:')\n",
    "    result['Disposition Desc'] = extract_field('Disposition:')\n",
    "    result['Disposition Date'] = extract_field('Disposition Date:')\n",
    "    result['Judgment Date'] = extract_field('Judgment Date:')\n",
    "    result['Claim Amount'] = extract_field('Claim Amount:')\n",
    "\n",
    "    # Parties\n",
    "    party_blocks = soup.select('#partyInfo div.even, #partyInfo div.odd')\n",
    "    for block in party_blocks:\n",
    "        role = block.find('span', string='Party Type:')\n",
    "        name = block.find('span', string='Party Name:')\n",
    "        if role and name:\n",
    "            role_value = role.find_next_sibling('span').text.strip()\n",
    "            name_value = name.find_next_sibling('span').text.strip()\n",
    "            if role_value == 'Plaintiff':\n",
    "                result['Plaintiff'] = name_value\n",
    "            elif role_value == 'Defendant':\n",
    "                result['Defendant'] = name_value\n",
    "\n",
    "    if not result.get('Filed Date') or not result.get('Plaintiff'):\n",
    "        result['Error'] = 'Missing core data'\n",
    "        return result\n",
    "\n",
    "    # Hearings (up to 10)\n",
    "    hearing_blocks = soup.select('#eventInfo div.even, #eventInfo div.odd')[:10]\n",
    "    for i, block in enumerate(hearing_blocks):\n",
    "        desc = block.find('span', string='Hearing Description:')\n",
    "        date = block.find('span', string='Hearing Date/Time:')\n",
    "        result_text = block.find_all('span')[-3].text.strip() if len(block.find_all('span')) > 3 else ''\n",
    "        result[f'Hearing {i+1} Description'] = desc.find_next_sibling('span').text.strip() if desc else ''\n",
    "        result[f'Hearing {i+1} Date'] = date.find_next_sibling('span').text.strip() if date else ''\n",
    "        result[f'Hearing {i+1} Result'] = result_text\n",
    "\n",
    "    # Events (up to 25)\n",
    "    event_blocks = soup.select('#filingInfo div.even, #filingInfo div.odd')[:25]\n",
    "    for i, block in enumerate(event_blocks):\n",
    "        desc = block.find('span', string=lambda s: s and 'Event' in s)\n",
    "        date = block.find('span', string='Date Added:')\n",
    "        result[f'Event {i+1} Description'] = desc.find_next_sibling('span').text.strip() if desc else ''\n",
    "        result[f'Event {i+1} Date'] = date.find_next_sibling('span').text.strip() if date else ''\n",
    "\n",
    "    return result\n",
    "\n",
    "# Scrape the sample\n",
    "results = []\n",
    "for _, row in tqdm(sampled_cases.iterrows(), total=len(sampled_cases), desc=\"Scraping eviction cases\"):\n",
    "    case_number = str(row['Case Number'])\n",
    "    court_number = row['court_number']\n",
    "    scraped = scrape_eviction_case(case_number, court_number)\n",
    "    results.append(scraped)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Save output\n",
    "df_scraped = pd.DataFrame(results)\n",
    "df_scraped.to_csv('Eviction_Cases_Sample_WithCourt.csv', index=False)\n",
    "print(\"✅ Scraping complete. Saved to 'Eviction_Cases_Sample_WithCourt.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "986c458c-fc3c-453c-b27e-9c662747b5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ferna\\AppData\\Local\\Temp\\ipykernel_14520\\283637755.py:9: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_cases = pd.read_csv('Harris_EV_Filings_2023_042025_CLEAN.csv', usecols=['Case Number', 'court_number'])\n",
      "Processing chunk 1: 100%|██████████████████████████████████████████████████████████████| 25/25 [00:18<00:00,  1.32it/s]\n",
      "Processing chunk 2: 100%|██████████████████████████████████████████████████████████████| 25/25 [00:19<00:00,  1.28it/s]\n",
      "Processing chunk 3: 100%|██████████████████████████████████████████████████████████████| 25/25 [00:19<00:00,  1.28it/s]\n",
      "Processing chunk 4: 100%|██████████████████████████████████████████████████████████████| 25/25 [00:18<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All chunks scraped and saved to 'Eviction_Cases_Sample_WithCourt.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Load your dataset and sample 100\n",
    "df_cases = pd.read_csv('Harris_EV_Filings_2023_042025_CLEAN.csv', usecols=['Case Number', 'court_number'])\n",
    "sampled_cases = df_cases.dropna(subset=['Case Number']).sample(n=100, random_state=42)\n",
    "\n",
    "session = requests.Session()\n",
    "output_file = 'Eviction_Cases_Sample_WithCourt.csv'\n",
    "\n",
    "# Remove old file if re-running\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "def scrape_eviction_case(case_number, court_number):\n",
    "    url = f'https://jpwebsite.harriscountytx.gov/CaseInfo/GetCaseInfo?case={case_number}'\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            response = session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            break\n",
    "        except requests.RequestException as e:\n",
    "            if attempt == 2:\n",
    "                return {'Case Number': case_number, 'court_number': court_number, 'Error': str(e)}\n",
    "            time.sleep(1)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    result = {'Case Number': case_number, 'court_number': court_number}\n",
    "\n",
    "    def extract_field(label):\n",
    "        tag = soup.find('span', string=label)\n",
    "        if tag and tag.find_next_sibling('span'):\n",
    "            return tag.find_next_sibling('span').text.strip()\n",
    "        return ''\n",
    "\n",
    "    result['Filed Date'] = extract_field('Filed Date:')\n",
    "    result['Case Status'] = extract_field('Case Status:')\n",
    "    result['Nature of Claim'] = extract_field('Nature of Claim:')\n",
    "    result['Disposition Desc'] = extract_field('Disposition:')\n",
    "    result['Disposition Date'] = extract_field('Disposition Date:')\n",
    "    result['Judgment Date'] = extract_field('Judgment Date:')\n",
    "    result['Claim Amount'] = extract_field('Claim Amount:')\n",
    "\n",
    "    # Parties\n",
    "    party_blocks = soup.select('#partyInfo div.even, #partyInfo div.odd')\n",
    "    for block in party_blocks:\n",
    "        role = block.find('span', string='Party Type:')\n",
    "        name = block.find('span', string='Party Name:')\n",
    "        if role and name:\n",
    "            role_value = role.find_next_sibling('span').text.strip()\n",
    "            name_value = name.find_next_sibling('span').text.strip()\n",
    "            if role_value == 'Plaintiff':\n",
    "                result['Plaintiff'] = name_value\n",
    "            elif role_value == 'Defendant':\n",
    "                result['Defendant'] = name_value\n",
    "\n",
    "    if not result.get('Filed Date') or not result.get('Plaintiff'):\n",
    "        result['Error'] = 'Missing core data'\n",
    "        return result\n",
    "\n",
    "    # Hearings (up to 10)\n",
    "    hearing_blocks = soup.select('#eventInfo div.even, #eventInfo div.odd')[:10]\n",
    "    for i, block in enumerate(hearing_blocks):\n",
    "        desc = block.find('span', string='Hearing Description:')\n",
    "        date = block.find('span', string='Hearing Date/Time:')\n",
    "        result_text = block.find_all('span')[-3].text.strip() if len(block.find_all('span')) > 3 else ''\n",
    "        result[f'Hearing {i+1} Description'] = desc.find_next_sibling('span').text.strip() if desc else ''\n",
    "        result[f'Hearing {i+1} Date'] = date.find_next_sibling('span').text.strip() if date else ''\n",
    "        result[f'Hearing {i+1} Result'] = result_text\n",
    "\n",
    "    # Events (up to 25)\n",
    "    event_blocks = soup.select('#filingInfo div.even, #filingInfo div.odd')[:25]\n",
    "    for i, block in enumerate(event_blocks):\n",
    "        desc = block.find('span', string=lambda s: s and 'Event' in s)\n",
    "        date = block.find('span', string='Date Added:')\n",
    "        result[f'Event {i+1} Description'] = desc.find_next_sibling('span').text.strip() if desc else ''\n",
    "        result[f'Event {i+1} Date'] = date.find_next_sibling('span').text.strip() if date else ''\n",
    "\n",
    "    return result\n",
    "\n",
    "# Scrape in chunks of 25\n",
    "chunk_size = 25\n",
    "for i in range(0, len(sampled_cases), chunk_size):\n",
    "    chunk = sampled_cases.iloc[i:i+chunk_size]\n",
    "    chunk_results = []\n",
    "    for _, row in tqdm(chunk.iterrows(), total=len(chunk), desc=f\"Processing chunk {i//chunk_size + 1}\"):\n",
    "        case_number = str(row['Case Number'])\n",
    "        court_number = row['court_number']\n",
    "        data = scrape_eviction_case(case_number, court_number)\n",
    "        chunk_results.append(data)\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    df_chunk = pd.DataFrame(chunk_results)\n",
    "    df_chunk.to_csv(output_file, mode='a', index=False, header=not os.path.exists(output_file))\n",
    "\n",
    "print(f\"✅ All chunks scraped and saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76a6ec4d-77a7-4bef-9690-fa39bfab508c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing court JP 1-1 — 12146 eligible cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Court JP 1-1 | Chunk 1:   3%|█▊                                                       | 16/500 [00:13<06:35,  1.22it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 103\u001b[0m\n\u001b[0;32m    101\u001b[0m case_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCase Number\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    102\u001b[0m court_number \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcourt_number\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 103\u001b[0m data \u001b[38;5;241m=\u001b[39m scrape_eviction_case(case_number, court_number)\n\u001b[0;32m    104\u001b[0m chunk_results\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[0;32m    105\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m, in \u001b[0;36mscrape_eviction_case\u001b[1;34m(case_number, court_number)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m         response \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget(url, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     30\u001b[0m         response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Load data and apply filters\n",
    "df_all = pd.read_csv('Harris_EV_Filings_2023_042025_CLEAN.csv', usecols=['Case Number', 'court_number', 'Case File Date', 'Case Status'], low_memory=False)\n",
    "df_all['Case File Date'] = pd.to_datetime(df_all['Case File Date'], errors='coerce')\n",
    "\n",
    "# Keep only cases from 2024 and 2025 that are not 'Active'\n",
    "df_filtered = df_all[\n",
    "    (df_all['Case File Date'].dt.year >= 2024) &\n",
    "    (df_all['Case File Date'].dt.year <= 2025) &\n",
    "    (df_all['Case Status'].str.strip().str.lower() != 'active')\n",
    "].dropna(subset=['Case Number']).drop_duplicates(subset=['Case Number'])\n",
    "\n",
    "session = requests.Session()\n",
    "output_file = 'Eviction_Cases_ByCourt_Sample20k.csv'\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "def scrape_eviction_case(case_number, court_number):\n",
    "    url = f'https://jpwebsite.harriscountytx.gov/CaseInfo/GetCaseInfo?case={case_number}'\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            response = session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            break\n",
    "        except requests.RequestException as e:\n",
    "            if attempt == 2:\n",
    "                return {'Case Number': case_number, 'court_number': court_number, 'Error': str(e)}\n",
    "            time.sleep(1)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    result = {'Case Number': case_number, 'court_number': court_number}\n",
    "\n",
    "    def extract_field(label):\n",
    "        tag = soup.find('span', string=label)\n",
    "        if tag and tag.find_next_sibling('span'):\n",
    "            return tag.find_next_sibling('span').text.strip()\n",
    "        return ''\n",
    "\n",
    "    result['Filed Date'] = extract_field('Filed Date:')\n",
    "    result['Case Status'] = extract_field('Case Status:')\n",
    "    result['Nature of Claim'] = extract_field('Nature of Claim:')\n",
    "    result['Disposition Desc'] = extract_field('Disposition:')\n",
    "    result['Disposition Date'] = extract_field('Disposition Date:')\n",
    "    result['Judgment Date'] = extract_field('Judgment Date:')\n",
    "    result['Claim Amount'] = extract_field('Claim Amount:')\n",
    "\n",
    "    party_blocks = soup.select('#partyInfo div.even, #partyInfo div.odd')\n",
    "    for block in party_blocks:\n",
    "        role = block.find('span', string='Party Type:')\n",
    "        name = block.find('span', string='Party Name:')\n",
    "        if role and name:\n",
    "            role_value = role.find_next_sibling('span').text.strip()\n",
    "            name_value = name.find_next_sibling('span').text.strip()\n",
    "            if role_value == 'Plaintiff':\n",
    "                result['Plaintiff'] = name_value\n",
    "            elif role_value == 'Defendant':\n",
    "                result['Defendant'] = name_value\n",
    "\n",
    "    if not result.get('Filed Date') or not result.get('Plaintiff'):\n",
    "        result['Error'] = 'Missing core data'\n",
    "        return result\n",
    "\n",
    "    hearing_blocks = soup.select('#eventInfo div.even, #eventInfo div.odd')[:10]\n",
    "    for i, block in enumerate(hearing_blocks):\n",
    "        desc = block.find('span', string='Hearing Description:')\n",
    "        date = block.find('span', string='Hearing Date/Time:')\n",
    "        result_text = block.find_all('span')[-3].text.strip() if len(block.find_all('span')) > 3 else ''\n",
    "        result[f'Hearing {i+1} Description'] = desc.find_next_sibling('span').text.strip() if desc else ''\n",
    "        result[f'Hearing {i+1} Date'] = date.find_next_sibling('span').text.strip() if date else ''\n",
    "        result[f'Hearing {i+1} Result'] = result_text\n",
    "\n",
    "    event_blocks = soup.select('#filingInfo div.even, #filingInfo div.odd')[:25]\n",
    "    for i, block in enumerate(event_blocks):\n",
    "        desc = block.find('span', string=lambda s: s and 'Event' in s)\n",
    "        date = block.find('span', string='Date Added:')\n",
    "        result[f'Event {i+1} Description'] = desc.find_next_sibling('span').text.strip() if desc else ''\n",
    "        result[f'Event {i+1} Date'] = date.find_next_sibling('span').text.strip() if date else ''\n",
    "\n",
    "    return result\n",
    "\n",
    "# Process per court\n",
    "chunk_size = 500\n",
    "grouped = df_filtered.groupby('court_number')\n",
    "\n",
    "for court, group_df in grouped:\n",
    "    print(f\"\\n📂 Processing court {court} — {len(group_df)} eligible cases\")\n",
    "    court_sample = group_df.sample(n=min(1500, len(group_df)), random_state=42)\n",
    "\n",
    "    for i in range(0, len(court_sample), chunk_size):\n",
    "        chunk = court_sample.iloc[i:i+chunk_size]\n",
    "        chunk_results = []\n",
    "\n",
    "        for _, row in tqdm(chunk.iterrows(), total=len(chunk), desc=f\"Court {court} | Chunk {i//chunk_size + 1}\"):\n",
    "            case_number = str(row['Case Number'])\n",
    "            court_number = row['court_number']\n",
    "            data = scrape_eviction_case(case_number, court_number)\n",
    "            chunk_results.append(data)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        df_chunk = pd.DataFrame(chunk_results)\n",
    "        df_chunk.to_csv(output_file, mode='a', index=False, header=not os.path.exists(output_file))\n",
    "\n",
    "print(f\"\\n✅ Scraping complete. All results saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a8bbea7-0a14-4995-9cb5-3b2aaf74dd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing court JP 1-1 — 12146 eligible cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Court JP 1-1 | Chunk 1: 100%|████████████████████████████████████████████████████████| 500/500 [06:19<00:00,  1.32it/s]\n",
      "Court JP 1-1 | Chunk 2: 100%|████████████████████████████████████████████████████████| 500/500 [06:19<00:00,  1.32it/s]\n",
      "Court JP 1-1 | Chunk 3: 100%|████████████████████████████████████████████████████████| 500/500 [06:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing court JP 1-2 — 1304 eligible cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Court JP 1-2 | Chunk 1: 100%|████████████████████████████████████████████████████████| 500/500 [06:14<00:00,  1.33it/s]\n",
      "Court JP 1-2 | Chunk 2: 100%|████████████████████████████████████████████████████████| 500/500 [06:21<00:00,  1.31it/s]\n",
      "Court JP 1-2 | Chunk 3: 100%|████████████████████████████████████████████████████████| 304/304 [03:47<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing court JP 2-1 — 3558 eligible cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Court JP 2-1 | Chunk 1: 100%|████████████████████████████████████████████████████████| 500/500 [06:22<00:00,  1.31it/s]\n",
      "Court JP 2-1 | Chunk 2: 100%|████████████████████████████████████████████████████████| 500/500 [06:24<00:00,  1.30it/s]\n",
      "Court JP 2-1 | Chunk 3: 100%|████████████████████████████████████████████████████████| 500/500 [06:27<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing court JP 2-2 — 1322 eligible cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Court JP 2-2 | Chunk 1: 100%|████████████████████████████████████████████████████████| 500/500 [06:33<00:00,  1.27it/s]\n",
      "Court JP 2-2 | Chunk 2: 100%|████████████████████████████████████████████████████████| 500/500 [06:33<00:00,  1.27it/s]\n",
      "Court JP 2-2 | Chunk 3: 100%|████████████████████████████████████████████████████████| 322/322 [04:15<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing court JP 3-1 — 5740 eligible cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Court JP 3-1 | Chunk 1: 100%|████████████████████████████████████████████████████████| 500/500 [06:22<00:00,  1.31it/s]\n",
      "Court JP 3-1 | Chunk 2: 100%|████████████████████████████████████████████████████████| 500/500 [06:27<00:00,  1.29it/s]\n",
      "Court JP 3-1 | Chunk 3: 100%|████████████████████████████████████████████████████████| 500/500 [06:28<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing court JP 3-2 — 1306 eligible cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Court JP 3-2 | Chunk 1: 100%|████████████████████████████████████████████████████████| 500/500 [06:31<00:00,  1.28it/s]\n",
      "Court JP 3-2 | Chunk 2: 100%|████████████████████████████████████████████████████████| 500/500 [06:20<00:00,  1.31it/s]\n",
      "Court JP 3-2 | Chunk 3: 100%|████████████████████████████████████████████████████████| 306/306 [03:55<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing court JP 4-1 — 15916 eligible cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Court JP 4-1 | Chunk 1: 100%|████████████████████████████████████████████████████████| 500/500 [06:26<00:00,  1.29it/s]\n",
      "Court JP 4-1 | Chunk 2: 100%|████████████████████████████████████████████████████████| 500/500 [06:22<00:00,  1.31it/s]\n",
      "Court JP 4-1 | Chunk 3: 100%|████████████████████████████████████████████████████████| 500/500 [06:23<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing court JP 4-2 — 6488 eligible cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Court JP 4-2 | Chunk 1: 100%|████████████████████████████████████████████████████████| 500/500 [06:30<00:00,  1.28it/s]\n",
      "Court JP 4-2 | Chunk 2: 100%|████████████████████████████████████████████████████████| 500/500 [06:20<00:00,  1.31it/s]\n",
      "Court JP 4-2 | Chunk 3: 100%|████████████████████████████████████████████████████████| 500/500 [06:27<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing court JP 5-1 — 14599 eligible cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Court JP 5-1 | Chunk 1: 100%|████████████████████████████████████████████████████████| 500/500 [06:20<00:00,  1.31it/s]\n",
      "Court JP 5-1 | Chunk 2: 100%|████████████████████████████████████████████████████████| 500/500 [06:25<00:00,  1.30it/s]\n",
      "Court JP 5-1 | Chunk 3: 100%|████████████████████████████████████████████████████████| 500/500 [06:26<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing court JP 5-2 — 13102 eligible cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Court JP 5-2 | Chunk 1: 100%|████████████████████████████████████████████████████████| 500/500 [06:33<00:00,  1.27it/s]\n",
      "Court JP 5-2 | Chunk 2: 100%|████████████████████████████████████████████████████████| 500/500 [06:45<00:00,  1.23it/s]\n",
      "Court JP 5-2 | Chunk 3: 100%|████████████████████████████████████████████████████████| 500/500 [06:42<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing court JP 6-1 — 1245 eligible cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Court JP 6-1 | Chunk 1: 100%|████████████████████████████████████████████████████████| 500/500 [06:43<00:00,  1.24it/s]\n",
      "Court JP 6-1 | Chunk 2: 100%|████████████████████████████████████████████████████████| 500/500 [06:38<00:00,  1.26it/s]\n",
      "Court JP 6-1 | Chunk 3: 100%|████████████████████████████████████████████████████████| 245/245 [03:09<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing court JP 6-2 — 1200 eligible cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Court JP 6-2 | Chunk 1: 100%|████████████████████████████████████████████████████████| 500/500 [06:31<00:00,  1.28it/s]\n",
      "Court JP 6-2 | Chunk 2: 100%|████████████████████████████████████████████████████████| 500/500 [06:33<00:00,  1.27it/s]\n",
      "Court JP 6-2 | Chunk 3: 100%|████████████████████████████████████████████████████████| 200/200 [02:36<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing court JP 7-1 — 3970 eligible cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Court JP 7-1 | Chunk 1: 100%|████████████████████████████████████████████████████████| 500/500 [06:40<00:00,  1.25it/s]\n",
      "Court JP 7-1 | Chunk 2: 100%|████████████████████████████████████████████████████████| 500/500 [06:34<00:00,  1.27it/s]\n",
      "Court JP 7-1 | Chunk 3: 100%|████████████████████████████████████████████████████████| 500/500 [06:39<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing court JP 7-2 — 7581 eligible cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Court JP 7-2 | Chunk 1: 100%|████████████████████████████████████████████████████████| 500/500 [06:24<00:00,  1.30it/s]\n",
      "Court JP 7-2 | Chunk 2: 100%|████████████████████████████████████████████████████████| 500/500 [06:21<00:00,  1.31it/s]\n",
      "Court JP 7-2 | Chunk 3: 100%|████████████████████████████████████████████████████████| 500/500 [06:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing court JP 8-1 — 3767 eligible cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Court JP 8-1 | Chunk 1: 100%|████████████████████████████████████████████████████████| 500/500 [06:23<00:00,  1.30it/s]\n",
      "Court JP 8-1 | Chunk 2: 100%|████████████████████████████████████████████████████████| 500/500 [06:16<00:00,  1.33it/s]\n",
      "Court JP 8-1 | Chunk 3: 100%|████████████████████████████████████████████████████████| 500/500 [06:16<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing court JP 8-2 — 1669 eligible cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Court JP 8-2 | Chunk 1: 100%|████████████████████████████████████████████████████████| 500/500 [06:16<00:00,  1.33it/s]\n",
      "Court JP 8-2 | Chunk 2: 100%|████████████████████████████████████████████████████████| 500/500 [06:09<00:00,  1.35it/s]\n",
      "Court JP 8-2 | Chunk 3: 100%|████████████████████████████████████████████████████████| 500/500 [06:08<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Scraping complete. All results saved to 'Eviction_Cases_ByCourt_Sample20k.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Load data and apply filters\n",
    "df_all = pd.read_csv('Harris_EV_Filings_2023_042025_CLEAN.csv', usecols=['Case Number', 'court_number', 'Case File Date', 'Case Status', 'Disposition Desc', 'Disposition Date', 'Judgment Date', 'Claim Amount'], low_memory=False)\n",
    "df_all['Case File Date'] = pd.to_datetime(df_all['Case File Date'], errors='coerce')\n",
    "\n",
    "# Keep only cases from 2024 and 2025 that are not 'Active'\n",
    "df_filtered = df_all[\n",
    "    (df_all['Case File Date'].dt.year >= 2024) &\n",
    "    (df_all['Case File Date'].dt.year <= 2025) &\n",
    "    (df_all['Case Status'].str.strip().str.lower() != 'active')\n",
    "].dropna(subset=['Case Number']).drop_duplicates(subset=['Case Number'])\n",
    "\n",
    "session = requests.Session()\n",
    "output_file = 'Eviction_Cases_ByCourt_Sample20k.csv'\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "def scrape_eviction_case(case_number, court_number, case_status, disp_desc, disp_date, judgment_date, claim_amount):\n",
    "    url = f'https://jpwebsite.harriscountytx.gov/CaseInfo/GetCaseInfo?case={case_number}'\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            response = session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            break\n",
    "        except requests.RequestException as e:\n",
    "            if attempt == 2:\n",
    "                return {'Case Number': case_number, 'court_number': court_number, 'Error': str(e)}\n",
    "            time.sleep(1)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    result = {'Case Number': case_number, 'court_number': court_number,\n",
    "              'Case Status': case_status, 'Disposition Desc': disp_desc,\n",
    "              'Disposition Date': disp_date, 'Judgment Date': judgment_date,\n",
    "              'Claim Amount': claim_amount}\n",
    "\n",
    "    # Scrape Hearings\n",
    "    hearing_blocks = soup.select('#eventInfo div.even, #eventInfo div.odd')[:10]\n",
    "    for i, block in enumerate(hearing_blocks):\n",
    "        desc = block.find('span', string='Hearing Description:')\n",
    "        date = block.find('span', string='Hearing Date/Time:')\n",
    "        result_text = block.find_all('span')[-3].text.strip() if len(block.find_all('span')) > 3 else ''\n",
    "        result[f'Hearing {i+1} Description'] = desc.find_next_sibling('span').text.strip() if desc else ''\n",
    "        result[f'Hearing {i+1} Date'] = date.find_next_sibling('span').text.strip() if date else ''\n",
    "        result[f'Hearing {i+1} Result'] = result_text\n",
    "\n",
    "    # Scrape Events\n",
    "    event_blocks = soup.select('#filingInfo div.even, #filingInfo div.odd')[:25]\n",
    "    for i, block in enumerate(event_blocks):\n",
    "        desc = block.find('span', string=lambda s: s and 'Event' in s)\n",
    "        date = block.find('span', string='Date Added:')\n",
    "        result[f'Event {i+1} Description'] = desc.find_next_sibling('span').text.strip() if desc else ''\n",
    "        result[f'Event {i+1} Date'] = date.find_next_sibling('span').text.strip() if date else ''\n",
    "\n",
    "    return result\n",
    "\n",
    "# Process per court\n",
    "chunk_size = 500\n",
    "grouped = df_filtered.groupby('court_number')\n",
    "\n",
    "for court, group_df in grouped:\n",
    "    print(f\"\\n📂 Processing court {court} — {len(group_df)} eligible cases\")\n",
    "    court_sample = group_df.sample(n=min(1500, len(group_df)), random_state=42)\n",
    "\n",
    "    for i in range(0, len(court_sample), chunk_size):\n",
    "        chunk = court_sample.iloc[i:i+chunk_size]\n",
    "        chunk_results = []\n",
    "\n",
    "        for _, row in tqdm(chunk.iterrows(), total=len(chunk), desc=f\"Court {court} | Chunk {i//chunk_size + 1}\"):\n",
    "            data = scrape_eviction_case(\n",
    "                row['Case Number'], row['court_number'],\n",
    "                row['Case Status'], row['Disposition Desc'],\n",
    "                row['Disposition Date'], row['Judgment Date'], row['Claim Amount']\n",
    "            )\n",
    "            chunk_results.append(data)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        df_chunk = pd.DataFrame(chunk_results)\n",
    "        df_chunk.to_csv(output_file, mode='a', index=False, header=not os.path.exists(output_file))\n",
    "\n",
    "print(f\"\\n✅ Scraping complete. All results saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00bce7c-243b-41cb-b92c-bc71b24332e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
